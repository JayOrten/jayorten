[
  {
    "id": "2725a1541a66",
    "title": "Attention Is All You Need",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ],
    "url": "http://arxiv.org/abs/1706.03762v7",
    "date_read": "2026-02-19",
    "tags": [],
    "notes": null,
    "rating": null,
    "source": "arxiv",
    "arxiv_id": "1706.03762",
    "doi": null
  },
  {
    "id": "43c3af5c1280",
    "title": "Attention Is All You Need",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ],
    "url": "http://arxiv.org/abs/1706.03762v7",
    "date_read": "2026-02-19",
    "tags": [],
    "notes": null,
    "rating": null,
    "source": "arxiv",
    "arxiv_id": "1706.03762",
    "doi": null
  },
  {
    "id": "c53cf997b0c5",
    "title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures",
    "authors": [
      "Hai Huang",
      "Yann LeCun",
      "Randall Balestriero"
    ],
    "url": "http://arxiv.org/abs/2509.14252v2",
    "date_read": "2026-02-19",
    "tags": [],
    "notes": null,
    "rating": null,
    "source": "arxiv",
    "arxiv_id": "2509.14252",
    "doi": null
  }
]
